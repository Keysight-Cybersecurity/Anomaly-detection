{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5f68bc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import joblib\n",
    "from pathlib import Path\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder, RobustScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "class AnomalyDetector:\n",
    "    def __init__(self):\n",
    "        self.trained_models = {}\n",
    "        self.best_model_info = None\n",
    "        self.scaler = None\n",
    "        self.label_encoders = {}\n",
    "        self.feature_columns = None\n",
    "        self.is_fitted = False\n",
    "        \n",
    "    def preprocess_data(self, data, is_training=True):\n",
    "        \"\"\"Preprocess data for training or prediction\"\"\"\n",
    "        data = data.copy()\n",
    "        \n",
    "        # Handle missing values\n",
    "        data = data.fillna(-1)\n",
    "        \n",
    "        # Convert data types\n",
    "        def convert_float_int_or_numeric_placeholder(val):\n",
    "            try:\n",
    "                f_val = float(val)\n",
    "                if f_val.is_integer():\n",
    "                    return int(f_val)\n",
    "                return f_val\n",
    "            except ValueError:\n",
    "                return str(val)\n",
    "        \n",
    "        data = data.applymap(convert_float_int_or_numeric_placeholder)\n",
    "        \n",
    "        # Drop non-useful columns\n",
    "        cols_to_drop = ['Time', 'ip_source', 'AMF_UE_NGAP_ID','EPD_2', 'spare_2', 'SecHdr_2', 'NASSecAlgo', \n",
    "                           'EPD_3', 'spare_3', 'SecHdr_3', 'Type_2',\n",
    "                           'PayloadContainerType', 'PayloadContainer', 'NAS_KSI', '5GSRegType',\n",
    "                            'RES',  'DeregistrationType' ]\n",
    "        \n",
    "        data_clean = data.drop(columns=cols_to_drop, errors='ignore')\n",
    "\n",
    "         # Store feature columns during training\n",
    "        if is_training:\n",
    "            self.feature_columns = data_clean.columns.tolist()\n",
    "        else:\n",
    "            # Ensure test data has same columns as training data\n",
    "            missing_cols = set(self.feature_columns) - set(data_clean.columns)\n",
    "            extra_cols = set(data_clean.columns) - set(self.feature_columns)\n",
    "            \n",
    "            if missing_cols:\n",
    "                print(f\"Warning: Missing columns in test data: {missing_cols}\")\n",
    "                for col in missing_cols:\n",
    "                    data_clean[col] = -1\n",
    "            \n",
    "            if extra_cols:\n",
    "                print(f\"Warning: Extra columns in test data (will be dropped): {extra_cols}\")\n",
    "                data_clean = data_clean.drop(columns=list(extra_cols))\n",
    "            \n",
    "            # Reorder columns to match training order\n",
    "            data_clean = data_clean[self.feature_columns]\n",
    "        \n",
    "        # Encode categorical variables\n",
    "        categorical_cols = data_clean.select_dtypes(include='object').columns\n",
    "        \n",
    "        for col in categorical_cols:\n",
    "            if is_training:\n",
    "                le = LabelEncoder()\n",
    "                data_clean[col] = le.fit_transform(data_clean[col].astype(str))\n",
    "                self.label_encoders[col] = le\n",
    "            else:\n",
    "                if col in self.label_encoders:\n",
    "                    le = self.label_encoders[col]\n",
    "                    def map_unknown(x):\n",
    "                        x_str = str(x)\n",
    "                        if x_str in le.classes_:\n",
    "                            return le.transform([x_str])[0]\n",
    "                        else:\n",
    "                            return -1\n",
    "                    data_clean[col] = data_clean[col].map(map_unknown)\n",
    "                else:\n",
    "                    data_clean[col] = -1\n",
    "        \n",
    "        # Scale features\n",
    "        if is_training:\n",
    "            self.scaler = RobustScaler()\n",
    "            X_scaled = self.scaler.fit_transform(data_clean)\n",
    "        else:\n",
    "            if self.scaler is None:\n",
    "                raise ValueError(\"Model must be fitted before prediction\")\n",
    "            X_scaled = self.scaler.transform(data_clean)\n",
    "        \n",
    "        return X_scaled, data_clean\n",
    "    \n",
    "    def calculate_anomaly_quality_score(self, anomaly_indices, X_data):\n",
    "        \"\"\"Calculate quality score for detected anomalies\"\"\"\n",
    "        if len(anomaly_indices) == 0:\n",
    "            return 0\n",
    "        \n",
    "        normal_indices = [i for i in range(len(X_data)) if i not in anomaly_indices]\n",
    "        if len(normal_indices) == 0:\n",
    "            return 0\n",
    "        \n",
    "        anomaly_data = X_data[anomaly_indices]\n",
    "        normal_data = X_data[normal_indices]\n",
    "        \n",
    "        # Statistical separation\n",
    "        anomaly_mean = np.mean(anomaly_data, axis=0)\n",
    "        normal_mean = np.mean(normal_data, axis=0)\n",
    "        separation_score = np.linalg.norm(anomaly_mean - normal_mean)\n",
    "        \n",
    "        # Anomaly cohesion\n",
    "        if len(anomaly_indices) > 1:\n",
    "            anomaly_variance = np.mean(np.var(anomaly_data, axis=0))\n",
    "            cohesion_score = 1 / (1 + anomaly_variance)\n",
    "        else:\n",
    "            cohesion_score = 1\n",
    "        \n",
    "        # Rarity score\n",
    "        rarity_score = 1 / (1 + len(anomaly_indices) / len(X_data))\n",
    "        \n",
    "        # Feature diversity\n",
    "        feature_differences = np.abs(anomaly_mean - normal_mean)\n",
    "        diversity_score = np.sum(feature_differences > np.std(feature_differences))\n",
    "        \n",
    "        total_score = (\n",
    "            0.3 * separation_score + \n",
    "            0.2 * cohesion_score + \n",
    "            0.3 * rarity_score + \n",
    "            0.2 * diversity_score\n",
    "        )\n",
    "        \n",
    "        return total_score\n",
    "\n",
    "    def get_parameter_grids(self, n_samples, n_features):\n",
    "        \"\"\"Get adaptive parameter grids based on data characteristics\"\"\"\n",
    "        return {\n",
    "            'lof': {\n",
    "                'n_neighbors': [max(10, n_samples//100), max(20, n_samples//50), max(30, n_samples//30)],\n",
    "                'contamination': [0.001, 0.005, 0.01, 0.02, 0.05]\n",
    "            },\n",
    "            'isolation_forest': {\n",
    "                'n_estimators': [100, 200],\n",
    "                'contamination': [0.001, 0.005, 0.01, 0.02, 0.05],\n",
    "                'max_features': [1.0, 0.8, min(n_features, 10)/n_features]\n",
    "            }\n",
    "        }\n",
    "\n",
    "    def train_isolation_forest(self, X_train, params):\n",
    "        \"\"\"Train and evaluate Isolation Forest models\"\"\"\n",
    "        results = []\n",
    "        print(f\"   Training {len(list(ParameterGrid(params)))} Isolation Forest configurations...\")\n",
    "        \n",
    "        for i, param_set in enumerate(ParameterGrid(params)):\n",
    "            try:\n",
    "                model = IsolationForest(\n",
    "                    n_estimators=param_set['n_estimators'],\n",
    "                    contamination=param_set['contamination'],\n",
    "                    max_features=param_set['max_features'],\n",
    "                    random_state=42\n",
    "                )\n",
    "                model.fit(X_train)\n",
    "                \n",
    "                # Evaluate on training data to get quality score\n",
    "                train_predictions = model.predict(X_train)\n",
    "                anomaly_indices = np.where(train_predictions == -1)[0]\n",
    "                \n",
    "                if len(anomaly_indices) > 0:\n",
    "                    quality_score = self.calculate_anomaly_quality_score(anomaly_indices, X_train)\n",
    "                    results.append({\n",
    "                        'method': 'IsolationForest',\n",
    "                        'model': model,\n",
    "                        'params': param_set,\n",
    "                        'quality_score': quality_score,\n",
    "                        'train_anomalies': len(anomaly_indices)\n",
    "                    })\n",
    "                    print(f\"     Config {i+1}: Found {len(anomaly_indices)} anomalies, score: {quality_score:.4f}\")\n",
    "                else:\n",
    "                    print(f\"     Config {i+1}: No anomalies detected\")\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"     Config {i+1}: Error - {str(e)}\")\n",
    "                continue\n",
    "\n",
    "        print(f\"   Successfully trained {len(results)} Isolation Forest models\")\n",
    "        return results\n",
    "    \n",
    "    def train_lof(self, X_train, params):\n",
    "        \"\"\"Train and evaluate LOF models\"\"\"\n",
    "        results = []\n",
    "        print(f\"   Training {len(list(ParameterGrid(params)))} LOF configurations...\")\n",
    "        \n",
    "        for i, param_set in enumerate(ParameterGrid(params)):\n",
    "            try:\n",
    "                # Check if we have enough samples for the n_neighbors parameter\n",
    "                if param_set['n_neighbors'] >= X_train.shape[0]:\n",
    "                    print(f\"     Config {i+1}: n_neighbors ({param_set['n_neighbors']}) >= n_samples ({X_train.shape[0]})\")\n",
    "                    continue\n",
    "                \n",
    "                model = LocalOutlierFactor(\n",
    "                    n_neighbors=param_set['n_neighbors'],\n",
    "                    contamination=param_set['contamination'],\n",
    "                    novelty=True  # Important for prediction on new data\n",
    "                )\n",
    "                model.fit(X_train)\n",
    "                \n",
    "                # Use decision function for evaluation (more reliable than outlier factors)\n",
    "                decision_scores = model.decision_function(X_train)\n",
    "                threshold = np.percentile(decision_scores, param_set['contamination'] * 100)\n",
    "                anomaly_indices = np.where(decision_scores < threshold)[0]\n",
    "                \n",
    "                if len(anomaly_indices) > 0:\n",
    "                    quality_score = self.calculate_anomaly_quality_score(anomaly_indices, X_train)\n",
    "                    results.append({\n",
    "                        'method': 'LOF',\n",
    "                        'model': model,\n",
    "                        'params': param_set,\n",
    "                        'quality_score': quality_score,\n",
    "                        'train_anomalies': len(anomaly_indices)\n",
    "                    })\n",
    "                    print(f\"     Config {i+1}: Found {len(anomaly_indices)} anomalies, score: {quality_score:.4f}\")\n",
    "                else:\n",
    "                    print(f\"     Config {i+1}: No anomalies detected\")\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"     Config {i+1}: Error - {str(e)}\")\n",
    "                continue\n",
    "        \n",
    "        print(f\"   Successfully trained {len(results)} LOF models\")\n",
    "        return results\n",
    "    \n",
    "    def print_anomaly_rows(self, anomaly_indices, X_data):\n",
    "        if len(anomaly_indices) == 0:\n",
    "            print(\"No anomalies detected.\")\n",
    "            return\n",
    "        \n",
    "        print(f\"Anomalies detected at rows: {anomaly_indices}\")\n",
    "        for idx in anomaly_indices:\n",
    "            print(f\"Row {idx}: {X_data[idx]}\")\n",
    "    \n",
    "    def detect_and_print_best_model_anomalies(self, results, X_scaled, X_clean): \n",
    "        if not results:\n",
    "            print(\"No models to evaluate.\")\n",
    "            return\n",
    "\n",
    "        # First, evaluate all models on test data to see which ones actually detect anomalies\n",
    "        model_evaluations = []\n",
    "        \n",
    "        print(\" Evaluating all models on test data...\")\n",
    "        for result in results:\n",
    "            try:\n",
    "                model = result['model']\n",
    "                method = result['method']\n",
    "                params = result['params']\n",
    "                train_quality = result['quality_score']\n",
    "                \n",
    "                # Test on actual test data\n",
    "                predictions = model.predict(X_scaled)\n",
    "                anomaly_indices = np.where(predictions == -1)[0]\n",
    "                test_anomaly_count = len(anomaly_indices)\n",
    "                \n",
    "                # Calculate test quality score if anomalies found\n",
    "                test_quality_score = 0\n",
    "                if test_anomaly_count > 0:\n",
    "                    test_quality_score = self.calculate_anomaly_quality_score(anomaly_indices, X_scaled)\n",
    "                \n",
    "                model_evaluations.append({\n",
    "                    'method': method,\n",
    "                    'model': model,\n",
    "                    'params': params,\n",
    "                    'train_quality': train_quality,\n",
    "                    'test_anomaly_count': test_anomaly_count,\n",
    "                    'test_quality_score': test_quality_score,\n",
    "                    'test_anomaly_indices': anomaly_indices\n",
    "                })\n",
    "                \n",
    "                # print(f\"   {method}: {test_anomaly_count} anomalies, test quality: {test_quality_score:.4f}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"   {result['method']}: Error - {str(e)}\")\n",
    "                continue\n",
    "        \n",
    "        # Filter models that actually detected anomalies\n",
    "        detecting_models = [m for m in model_evaluations if m['test_anomaly_count'] > 0]\n",
    "        \n",
    "        if not detecting_models:\n",
    "            print(\"\\n No models detected any anomalies on test data!\")\n",
    "            print(\"This could indicate:\")\n",
    "            print(\"   - The test data contains no anomalies\")\n",
    "            print(\"   - The contamination parameters are too strict\")\n",
    "            print(\"   - The models need different parameter tuning\")\n",
    "            return\n",
    "        \n",
    "        # Choose best model among those that actually detected anomalies\n",
    "        # Primary criterion: test quality score, secondary: fewer but higher quality anomalies\n",
    "        chosen_model = max(detecting_models, key=lambda x: (x['test_quality_score'], -x['test_anomaly_count']/1000))\n",
    "        \n",
    "        print(f\"\\nBest performing model: {chosen_model['method']}\")\n",
    "        print(f\"   - Test anomalies found: {chosen_model['test_anomaly_count']}\")\n",
    "        print(f\"   - Test quality score: {chosen_model['test_quality_score']:.4f}\")\n",
    "        print(f\"   - Training quality score: {chosen_model['train_quality']:.4f}\")\n",
    "        \n",
    "        # Show the anomalies from the best model\n",
    "        anomaly_indices = chosen_model['test_anomaly_indices']\n",
    "        print(f\"\\n ANOMALIES DETECTED BY BEST MODEL ({chosen_model['method']}):\")\n",
    "        print(f\"Anomaly indices: {list(anomaly_indices)}\")\n",
    "        print(\"\\nAnomalous rows:\")\n",
    "        print(X_clean.iloc[anomaly_indices])\n",
    "        \n",
    "        # Show comparison with other detecting models (group by method to avoid duplicates)\n",
    "        other_detecting = [m for m in detecting_models if m['method'] != chosen_model['method']]\n",
    "        if other_detecting:\n",
    "            print(f\"\\n\" + \"=\"*50)\n",
    "            print(\"COMPARISON WITH OTHER DETECTING MODELS:\")\n",
    "            print(\"=\"*50)\n",
    "            \n",
    "            # Group by method to avoid showing duplicates\n",
    "            methods_shown = set()\n",
    "            for model_eval in other_detecting:\n",
    "                method_name = model_eval['method']\n",
    "                if method_name in methods_shown:\n",
    "                    continue\n",
    "                methods_shown.add(method_name)\n",
    "                \n",
    "                print(f\"\\nüîç {method_name}:\")\n",
    "                print(f\"   - Anomalies: {model_eval['test_anomaly_count']}\")\n",
    "                print(f\"   - Quality score: {model_eval['test_quality_score']:.4f}\")\n",
    "                print(f\"   - Indices: {list(model_eval['test_anomaly_indices'])}\")\n",
    "                \n",
    "                # Show if there are different anomalies\n",
    "                if not np.array_equal(model_eval['test_anomaly_indices'], anomaly_indices):\n",
    "                    print(\"   - Different anomalies detected:\")\n",
    "                    print(X_clean.iloc[model_eval['test_anomaly_indices']])\n",
    "                else:\n",
    "                    print(\"   - Same anomalies as best model\")\n",
    "        \n",
    "        # Show models that didn't detect anything (group by method to avoid duplicates)\n",
    "        non_detecting = [m for m in model_evaluations if m['test_anomaly_count'] == 0]\n",
    "        if non_detecting:\n",
    "            print(f\"\\nüìã Models that found no anomalies: {len(non_detecting)}\")\n",
    "            methods_shown = set()\n",
    "            count_shown = 0\n",
    "            for m in non_detecting:\n",
    "                if m['method'] not in methods_shown and count_shown < 3:\n",
    "                    methods_shown.add(m['method'])\n",
    "                    print(f\"   - {m['method']} (train quality: {m['train_quality']:.4f})\")\n",
    "                    count_shown += 1\n",
    "            remaining = len([m for m in non_detecting if m['method'] not in methods_shown])\n",
    "            if remaining > 0:\n",
    "                print(f\"   - ... and {remaining} other configurations\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "01ce44da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Phase 1: Training\n",
      "   Training 30 Isolation Forest configurations...\n",
      "     Config 1: Found 1 anomalies, score: 16.2429\n",
      "     Config 2: Found 1 anomalies, score: 16.2429\n",
      "     Config 3: Found 1 anomalies, score: 3.3592\n",
      "     Config 4: Found 1 anomalies, score: 3.3592\n",
      "     Config 5: Found 1 anomalies, score: 16.2429\n",
      "     Config 6: Found 1 anomalies, score: 16.2429\n",
      "     Config 7: Found 2 anomalies, score: 9.5967\n",
      "     Config 8: Found 2 anomalies, score: 9.5967\n",
      "     Config 9: Found 2 anomalies, score: 9.5967\n",
      "     Config 10: Found 2 anomalies, score: 9.5967\n",
      "     Config 11: Found 2 anomalies, score: 9.5967\n",
      "     Config 12: Found 2 anomalies, score: 9.5967\n",
      "     Config 13: Found 2 anomalies, score: 9.5967\n",
      "     Config 14: Found 2 anomalies, score: 9.5967\n",
      "     Config 15: Found 2 anomalies, score: 9.5967\n",
      "     Config 16: Found 2 anomalies, score: 9.5967\n",
      "     Config 17: Found 2 anomalies, score: 9.5967\n",
      "     Config 18: Found 2 anomalies, score: 9.5967\n",
      "     Config 19: Found 2 anomalies, score: 9.5967\n",
      "     Config 20: Found 2 anomalies, score: 9.5967\n",
      "     Config 21: Found 2 anomalies, score: 9.5967\n",
      "     Config 22: Found 2 anomalies, score: 9.5967\n",
      "     Config 23: Found 2 anomalies, score: 9.5967\n",
      "     Config 24: Found 2 anomalies, score: 9.5967\n",
      "     Config 25: Found 2 anomalies, score: 9.5967\n",
      "     Config 26: Found 2 anomalies, score: 9.5967\n",
      "     Config 27: Found 2 anomalies, score: 9.5967\n",
      "     Config 28: Found 2 anomalies, score: 9.5967\n",
      "     Config 29: Found 2 anomalies, score: 9.5967\n",
      "     Config 30: Found 2 anomalies, score: 9.5967\n",
      "   Successfully trained 30 Isolation Forest models\n",
      "   Training 15 LOF configurations...\n",
      "     Config 1: Found 1 anomalies, score: 16.2429\n",
      "     Config 2: Found 1 anomalies, score: 16.2429\n",
      "     Config 3: Found 1 anomalies, score: 16.2429\n",
      "     Config 4: Found 2 anomalies, score: 9.5967\n",
      "     Config 5: Found 2 anomalies, score: 9.5967\n",
      "     Config 6: Found 2 anomalies, score: 9.5967\n",
      "     Config 7: Found 2 anomalies, score: 9.5967\n",
      "     Config 8: Found 2 anomalies, score: 9.5967\n",
      "     Config 9: Found 2 anomalies, score: 9.5967\n",
      "     Config 10: Found 2 anomalies, score: 9.5967\n",
      "     Config 11: Found 2 anomalies, score: 9.5967\n",
      "     Config 12: Found 2 anomalies, score: 9.5967\n",
      "     Config 13: Found 2 anomalies, score: 9.5967\n",
      "     Config 14: Found 2 anomalies, score: 9.5967\n",
      "     Config 15: Found 2 anomalies, score: 9.5967\n",
      "   Successfully trained 15 LOF models\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    print(\"Phase 1: Training\")\n",
    "    train_data = pd.read_csv(\"replaydata.csv\", dtype=str, sep=\";\")\n",
    "    detector = AnomalyDetector()\n",
    "\n",
    "    X_train, X_test = train_test_split(train_data, test_size=0.3, random_state=42)\n",
    "\n",
    "    # Preprocess training data first - this sets feature_columns, scaler, label encoders\n",
    "    X_train_scaled, X_train_clean = detector.preprocess_data(X_train, is_training=True)\n",
    "\n",
    "    # Now preprocess test data\n",
    "    X_test_scaled, X_test_clean = detector.preprocess_data(X_test, is_training=False)\n",
    "\n",
    "\n",
    "    # Get parameter grids based on training data shape\n",
    "    param_grids = detector.get_parameter_grids(n_samples=X_train_scaled.shape[0], n_features=X_train_scaled.shape[1])\n",
    "\n",
    "    # Train models\n",
    "    isolation_results = detector.train_isolation_forest(X_train_scaled, param_grids['isolation_forest'])\n",
    "    lof_results = detector.train_lof(X_train_scaled, param_grids['lof'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "204d8a69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "ANOMALY DETECTION SYSTEM\n",
      "============================================================\n",
      "\n",
      "Phase 1: Loading and Splitting Data\n",
      "----------------------------------------\n",
      " Data loaded successfully: 910 rows, 25 columns\n",
      " Data split: 637 training samples, 273 test samples\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"=\"*60)\n",
    "    print(\"ANOMALY DETECTION SYSTEM\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    print(\"\\nPhase 1: Loading and Splitting Data\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    try:\n",
    "        train_data = pd.read_csv(\"replaydata.csv\", dtype=str, sep=\";\")\n",
    "        print(f\" Data loaded successfully: {train_data.shape[0]} rows, {train_data.shape[1]} columns\")\n",
    "        \n",
    "        detector = AnomalyDetector()\n",
    "        X_train, X_test = train_test_split(train_data, test_size=0.3, random_state=42)\n",
    "        print(f\" Data split: {X_train.shape[0]} training samples, {X_test.shape[0]} test samples\")\n",
    "        \n",
    "    except FileNotFoundError:\n",
    "        print(\" Error: 'replaydata.csv' not found in current directory\")\n",
    "        exit(1)\n",
    "    except Exception as e:\n",
    "        print(f\" Error loading data: {e}\")\n",
    "        exit(1)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "529bf000",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Phase 2: Data Preprocessing\n",
      "----------------------------------------\n",
      " Training data preprocessed: 8 features\n",
      " Test data preprocessed: 8 features\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nPhase 2: Data Preprocessing\")\n",
    "print(\"-\" * 40)\n",
    "    \n",
    "try:\n",
    "    # Preprocess training data first - this sets feature_columns, scaler, label encoders\n",
    "    X_train_scaled, X_train_clean = detector.preprocess_data(X_train, is_training=True)\n",
    "    print(f\" Training data preprocessed: {X_train_scaled.shape[1]} features\")\n",
    "    \n",
    "    # Now preprocess test data\n",
    "    X_test_scaled, X_test_clean = detector.preprocess_data(X_test, is_training=False)\n",
    "    print(f\" Test data preprocessed: {X_test_scaled.shape[1]} features\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\" Error during preprocessing: {e}\")\n",
    "    exit(1)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "91a90811",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Phase 4: Anomaly Detection and Evaluation\n",
      "----------------------------------------\n",
      " Evaluating all models on test data...\n",
      "\n",
      "Best performing model: IsolationForest\n",
      "   - Test anomalies found: 2\n",
      "   - Test quality score: 16.3516\n",
      "   - Training quality score: 9.5967\n",
      "\n",
      " ANOMALIES DETECTED BY BEST MODEL (IsolationForest):\n",
      "Anomaly indices: [np.int64(8), np.int64(148)]\n",
      "\n",
      "Anomalous rows:\n",
      "     procedureCode  EPD  spare  SecHdr  Type  5GSID  UESecCap  Seqn\n",
      "905             46  126      0       4    94     54         2     0\n",
      "904             46  126      0       4    94     54         2     0\n",
      "\n",
      "==================================================\n",
      "COMPARISON WITH OTHER DETECTING MODELS:\n",
      "==================================================\n",
      "\n",
      "üîç LOF:\n",
      "   - Anomalies: 2\n",
      "   - Quality score: 16.3516\n",
      "   - Indices: [np.int64(8), np.int64(148)]\n",
      "   - Same anomalies as best model\n",
      "\n",
      "üìã Models that found no anomalies: 6\n",
      "   - IsolationForest (train quality: 16.2429)\n",
      "\n",
      "============================================================\n",
      "ANALYSIS COMPLETE\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nPhase 4: Anomaly Detection and Evaluation\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "try:\n",
    "    # Combine all results and detect anomalies\n",
    "    all_results = isolation_results + lof_results\n",
    "    \n",
    "    if all_results:\n",
    "        # print(f\" Total trained models: {len(all_results)}\")\n",
    "        detector.detect_and_print_best_model_anomalies(all_results, X_test_scaled, X_test_clean)\n",
    "    else:\n",
    "        print(\" No models were successfully trained\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\" Error during anomaly detection: {e}\")\n",
    "    exit(1)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ANALYSIS COMPLETE\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37acfb97",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25aa931a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
